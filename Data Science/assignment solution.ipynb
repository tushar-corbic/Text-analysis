{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary files\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import bs4 as bs\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from selenium import webdriver\n",
    "import re\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselink = 'https://www.sec.gov/Archives/'# defining the baselink \n",
    "cik_list = pd.read_csv('cik_list.csv').fillna(\"\") # opening the cik_list file\n",
    "links =[ i for i in cik_list['SECFNAME'].tolist() if i!=\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dict = pd.read_csv('LoughranMcDonald_MasterDictionary_2020.csv')\n",
    "positive_words = master_dict[master_dict['Positive']==2009]['Word'] # find the positive words from the master dictionary\n",
    "negative_words = master_dict[master_dict['Negative']==2009]['Word'] # finding the negative words from the master dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = list(map(lambda x:x.lower(), positive_words))\n",
    "negative_words = list(map(lambda x:x.lower(), negative_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainity_dictionary = pd.read_csv('uncertainty_dictionary.csv')['Word']\n",
    "# making a list of words in the uncertinaity dictioary\n",
    "uncertainity_dictionary = list(map(lambda x : x.lower(), uncertainity_dictionary)) \n",
    "\n",
    "\n",
    "constraining_dictionary = pd.read_csv('constraining_dictionary.csv')['Word']\n",
    "# making the list of words in the constraning dictionary \n",
    "constraining_dictionary = list(map(lambda x: x.lower(), constraining_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words_file_names = ['StopWords_GenericLong.txt','StopWords_Generic.txt','StopWords_Auditor.txt','StopWords_Names.txt']\n",
    "stop_words=[]\n",
    "for i in stop_words_file_names:\n",
    "    file= pd.read_csv(i, header = None)\n",
    "    stop_words.extend(list(map(lambda x: x.lower() if isinstance(x,str) else x, file.iloc[:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= pd.read_csv('Output Data Structure.csv')\n",
    "col = dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(word):\n",
    "    \"\"\"\n",
    "        count the number of syllables in the word\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store the values for each document\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_for_each_text(text, no_of_sentences, cil, coname,fyrmo,fdate,form,secfname):\n",
    "    \"\"\"input-> list of cleaned sentences scrapped from a particular link\n",
    "        rest input is the data about the file corresponding to that link\n",
    "    \"\"\"\n",
    "    # positive_score, negetive_score\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "    for i in text:\n",
    "        for j in i.split():\n",
    "            if j in negative_words:\n",
    "                negative_score += 1\n",
    "            elif j in positive_words:\n",
    "                positive_score +=1\n",
    "    # polarity_score\n",
    "    polarity_score = (positive_score-negative_score)/(positive_score+negative_score+0.000001)\n",
    "    \n",
    "    subjectivity_score = (positive_score+negative_score)/(len(text)+0.000001)\n",
    "    \n",
    "    word_count = 0\n",
    "    for i in text:\n",
    "        word_count += len(i.split())\n",
    "  \n",
    "    # average_sentence_length\n",
    "    average_sentence_length = word_count/no_of_sentences\n",
    "    \n",
    "    no_complex_words = 0\n",
    "    for i in text:\n",
    "        for j in i.split():\n",
    "            if syllable_count(j)>2:\n",
    "                no_complex_words+=1\n",
    "    percentage_complex_words = no_complex_words/len(text)\n",
    "    \n",
    "    #fog index\n",
    "    fog_index = 0.4*(percentage_complex_words+average_sentence_length)\n",
    "    # word count\n",
    "    \n",
    "    # uncertainity\n",
    "    uncertainity = 0\n",
    "    constraining=0\n",
    "    for i in text:\n",
    "        for j in i.split():\n",
    "            if j in uncertainity_dictionary:\n",
    "                uncertainity +=1\n",
    "            if j in constraining_dictionary:\n",
    "                constraining+=1\n",
    "                \n",
    "                \n",
    "    positive_word_proportion = positive_score/no_of_sentences\n",
    "    negative_word_proportion = negative_score/no_of_sentences\n",
    "    constraining_word_proportion = constraining/no_of_sentences\n",
    "    uncertainity_word_proportion = uncertainity/no_of_sentences\n",
    "    \n",
    "    data.append([cil,coname,fyrmo,fdate,form,secfname,positive_score,\\\n",
    "                             negative_score,polarity_score,average_sentence_length,\\\n",
    "                             percentage_complex_words,fog_index,no_complex_words,word_count,uncertainity,\\\n",
    "                             constraining,positive_word_proportion,negative_word_proportion,\\\n",
    "                             uncertainity_word_proportion, constraining_word_proportion,constraining])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"the takes a list of sentences for each each document\n",
    "return the cleaned list of sentences where each sentence is manipulated as follows\n",
    "converted to smaller case, then removed the html tags, removed consecutive white spaces, removed special characters\n",
    "then tokenized the sentence, removed the stop words, stemmed the words,  return the clean sentences  \n",
    "\"\"\"\n",
    "def preprocess(sentences):\n",
    "    ps = PorterStemmer()\n",
    "    cleaned_sentences=[]\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    print(\"the number of sentences to be preprocessed \", len(sentences))\n",
    "    for x in sentences:\n",
    "        x = x.lower()\n",
    "        re.sub(r'<[^>]*>',' ', x)\n",
    "        re.sub(r'\\s+','', x)\n",
    "        re.sub(r'\\W',' ', x)\n",
    "        x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
    "                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
    "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
    "                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
    "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n",
    "        x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n",
    "        x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n",
    "        x = tokenizer.tokenize(x)\n",
    "        x = [ i for i in x if i not in stop_words]\n",
    "        x = [ ps.stem(i) for i in x]\n",
    "        cleaned_sentence = \" \".join(x)\n",
    "        cleaned_sentences.append(cleaned_sentence)\n",
    "        \n",
    "    return cleaned_sentences\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"this function takes the full URL of the file and then extracts the data and returns list of sentences \"\"\"\n",
    "all_text = []\n",
    "\n",
    "def get_text(full_link):\n",
    "    time.sleep(15)\n",
    "    driver.get(full_link)\n",
    "    arr = driver.find_elements_by_tag_name('body')\n",
    "\n",
    "   \n",
    "    soup= bs.BeautifulSoup(arr[0].text, 'lxml')\n",
    "    \n",
    "    \n",
    "    so=soup.find_all('page')\n",
    "    sentences = []\n",
    "    for i in range(len(so)):\n",
    "        sentences.extend([j.strip() for j in so[i].text.strip().split('\\n') if j!=''])\n",
    "    \n",
    "    if len(sentences)==0:\n",
    "        so = soup.find_all('text')\n",
    "        for i in range(len(so)):\n",
    "            sentences.extend([j.strip() for j in so[i].text.strip().split('\\n') if j!=''])       \n",
    "    clean_sentences = preprocess(sentences)\n",
    "    all_text.append(clean_sentences)\n",
    "    return clean_sentences\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/6201/0000950134-01-500665.txt\n",
      "the number of sentences to be preprocessed  8935\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000006201-01-500032.txt\n",
      "the number of sentences to be preprocessed  14918\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000006201-01-500047.txt\n",
      "the number of sentences to be preprocessed  23499\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000950134-02-001661.txt\n",
      "the number of sentences to be preprocessed  159769\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000006201-02-000015.txt\n",
      "the number of sentences to be preprocessed  7626\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000006201-02-000035.txt\n",
      "the number of sentences to be preprocessed  25817\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000006201-02-000052.txt\n",
      "the number of sentences to be preprocessed  25289\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000950134-02-012680.txt\n",
      "the number of sentences to be preprocessed  10159\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000950134-02-012682.txt\n",
      "the number of sentences to be preprocessed  16351\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000006201-03-000012.txt\n",
      "the number of sentences to be preprocessed  189\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0001047469-03-013301.txt\n",
      "page does not exit\n",
      "22981\n",
      "the number of sentences to be preprocessed  22981\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000006201-03-000030.txt\n",
      "the number of sentences to be preprocessed  23390\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000006201-03-000045.txt\n",
      "the number of sentences to be preprocessed  37637\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000006201-08-000060.txt\n",
      "page does not exit\n",
      "11199\n",
      "the number of sentences to be preprocessed  11199\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000006201-09-000009.txt\n",
      "page does not exit\n",
      "41678\n",
      "the number of sentences to be preprocessed  41678\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000006201-09-000016.txt\n",
      "page does not exit\n",
      "4755\n",
      "the number of sentences to be preprocessed  4755\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000006201-09-000029.txt\n",
      "page does not exit\n",
      "12150\n",
      "the number of sentences to be preprocessed  12150\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000006201-09-000038.txt\n",
      "page does not exit\n",
      "5375\n",
      "the number of sentences to be preprocessed  5375\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000006201-09-000040.txt\n",
      "page does not exit\n",
      "341\n",
      "the number of sentences to be preprocessed  341\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000006201-10-000006.txt\n",
      "page does not exit\n",
      "17434\n",
      "the number of sentences to be preprocessed  17434\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000006201-10-000013.txt\n",
      "page does not exit\n",
      "3446\n",
      "the number of sentences to be preprocessed  3446\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000950123-10-066894.txt\n",
      "page does not exit\n",
      "51340\n",
      "the number of sentences to be preprocessed  51340\n",
      "https://www.sec.gov/Archives/edgar/data/6201/0000950123-10-094605.txt\n",
      "page does not exit\n",
      "23496\n",
      "the number of sentences to be preprocessed  23496\n",
      "https://www.sec.gov/Archives/edgar/data/6260/0000006260-94-000014.txt\n",
      "the number of sentences to be preprocessed  7892\n",
      "https://www.sec.gov/Archives/edgar/data/6260/0000006260-94-000016.txt\n",
      "the number of sentences to be preprocessed  2837\n",
      "https://www.sec.gov/Archives/edgar/data/6260/0000006260-97-000011.txt\n",
      "the number of sentences to be preprocessed  59069\n",
      "https://www.sec.gov/Archives/edgar/data/6260/0000006260-98-000001.txt\n",
      "the number of sentences to be preprocessed  5368\n",
      "https://www.sec.gov/Archives/edgar/data/6260/0000006260-98-000003.txt\n",
      "the number of sentences to be preprocessed  2594\n",
      "https://www.sec.gov/Archives/edgar/data/6260/0000914121-98-000672.txt\n",
      "the number of sentences to be preprocessed  5407\n",
      "https://www.sec.gov/Archives/edgar/data/6260/0001047469-98-045227.txt\n",
      "the number of sentences to be preprocessed  97802\n",
      "https://www.sec.gov/Archives/edgar/data/6260/0000006260-99-000005.txt\n",
      "the number of sentences to be preprocessed  4415\n",
      "https://www.sec.gov/Archives/edgar/data/6260/0000006260-99-000007.txt\n",
      "the number of sentences to be preprocessed  8384\n",
      "https://www.sec.gov/Archives/edgar/data/6260/0000006260-99-000010.txt\n",
      "the number of sentences to be preprocessed  8955\n",
      "https://www.sec.gov/Archives/edgar/data/11860/0000011860-98-000022.txt\n",
      "the number of sentences to be preprocessed  31785\n",
      "https://www.sec.gov/Archives/edgar/data/11860/0001021408-99-000543.txt\n",
      "the number of sentences to be preprocessed  206945\n",
      "https://www.sec.gov/Archives/edgar/data/11860/0000011860-99-000025.txt\n",
      "the number of sentences to be preprocessed  4673\n",
      "https://www.sec.gov/Archives/edgar/data/11860/0000011860-99-000030.txt\n",
      "the number of sentences to be preprocessed  5512\n",
      "https://www.sec.gov/Archives/edgar/data/11860/0000011860-99-000035.txt\n",
      "the number of sentences to be preprocessed  4610\n",
      "https://www.sec.gov/Archives/edgar/data/11860/0000011860-99-000042.txt\n",
      "the number of sentences to be preprocessed  54053\n",
      "https://www.sec.gov/Archives/edgar/data/11860/0000011860-00-000019.txt\n",
      "the number of sentences to be preprocessed  695574\n",
      "https://www.sec.gov/Archives/edgar/data/11860/0000011860-00-000022.txt\n",
      "the number of sentences to be preprocessed  3079\n",
      "https://www.sec.gov/Archives/edgar/data/11860/0000011860-00-000025.txt\n",
      "the number of sentences to be preprocessed  4651\n",
      "https://www.sec.gov/Archives/edgar/data/11860/0000011860-00-000028.txt\n",
      "the number of sentences to be preprocessed  3497\n",
      "https://www.sec.gov/Archives/edgar/data/11860/0000011860-00-000038.txt\n",
      "the number of sentences to be preprocessed  3460\n",
      "https://www.sec.gov/Archives/edgar/data/12239/0001104659-07-024804.txt\n",
      "page does not exit\n",
      "8396\n",
      "the number of sentences to be preprocessed  8396\n",
      "https://www.sec.gov/Archives/edgar/data/12239/0001104659-07-040463.txt\n",
      "page does not exit\n",
      "275\n",
      "the number of sentences to be preprocessed  275\n",
      "https://www.sec.gov/Archives/edgar/data/12239/0001104659-07-041441.txt\n",
      "page does not exit\n",
      "1733\n",
      "the number of sentences to be preprocessed  1733\n",
      "https://www.sec.gov/Archives/edgar/data/12239/0001104659-07-042333.txt\n",
      "page does not exit\n",
      "8446\n",
      "the number of sentences to be preprocessed  8446\n",
      "https://www.sec.gov/Archives/edgar/data/12239/0001104659-07-062470.txt\n",
      "page does not exit\n",
      "2847\n",
      "the number of sentences to be preprocessed  2847\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(executable_path ='C:/Users/91885/Desktop/chromedriver')\n",
    "driver.maximize_window()\n",
    "\"\"\"Using  Selenium as the web driver to go to each link of web page\"\"\"\n",
    "for i in range(len(links)):\n",
    "    # forming the working link\n",
    "    full_link = baselink+links[i]\n",
    "    print(full_link)\n",
    "    document= get_text(full_link)\n",
    "    value_for_each_text(document, len(document), cik_list.iloc[i]['CIK'],cik_list.iloc[i]['CONAME'],cik_list.iloc[i]['FYRMO'], cik_list.iloc[i]['FDATE'],cik_list.iloc[i]['FORM'], cik_list.iloc[i]['SECFNAME'] )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Here I am making a CSV file which stores the result of each file link\"\"\"\n",
    "\n",
    "with open('Output Data Strucuture to be submitted.xlsx', 'w', encoding='UTF8', newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write the header\n",
    "    writer.writerow(col)\n",
    "\n",
    "    # write the data\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
